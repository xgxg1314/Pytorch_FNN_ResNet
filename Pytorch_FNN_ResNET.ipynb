{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_FNN_ResNET.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXe++/HJtjuD1rWUWt20Vd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xgxg1314/Pytorch_FNN_ResNet/blob/main/Pytorch_FNN_ResNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7HHeGxZjNvq"
      },
      "outputs": [],
      "source": [
        "# the deep neural network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class FNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        super(FNN, self).__init__()\n",
        "        \n",
        "        # parameters\n",
        "        self.depth = len(layers) - 1\n",
        "        \n",
        "        # set up layer order dict\n",
        "        #  torch.nn.Tanhshrink/torch.nn.Tanh\n",
        "        # torch.nn.functional.tanh\n",
        "        self.activation = torch.nn.Tanh\n",
        "        \n",
        "        layer_list = list()\n",
        "        for i in range(self.depth - 1): \n",
        "            layer_list.append(\n",
        "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
        "            )\n",
        "            layer_list.append(('activation_%d' % i, self.activation()))\n",
        "            \n",
        "        layer_list.append(\n",
        "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
        "        )\n",
        "        layerDict = OrderedDict(layer_list)\n",
        "        \n",
        "        # deploy layers\n",
        "        self.layers = torch.nn.Sequential(layerDict)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "class ResNN(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(ResNN, self).__init__()\n",
        "\n",
        "    self.hidden_layer1 = torch.nn.Linear(input_size, hidden_size)\n",
        "\n",
        "    self.hidden_layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "    self.hidden_layer3 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    # self.hidden_layer4 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "    # self.hidden_layer5 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # inputs = torch.cat([x,t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns\n",
        "\n",
        "    layer1_out = torch.tanh(self.hidden_layer1(inputs))\n",
        "\n",
        "    layer2_out = torch.tanh(self.hidden_layer2(layer1_out)) + layer1_out \n",
        "\n",
        "    layer3_out = torch.tanh(self.hidden_layer3(layer2_out)) + layer2_out \n",
        "\n",
        "    # layer4_out = torch.tanh(self.hidden_layer4(layer3_out)) + layer3_out \n",
        "    # layer5_out = torch.tanh(self.hidden_layer5(layer4_out)) + layer4_out \n",
        "\n",
        "    output = self.output_layer(layer3_out) ## For regression, no activation is used in output layer\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "3SAvMPTAjQ7P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}